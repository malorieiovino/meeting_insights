# -*- coding: utf-8 -*-
"""MiniProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QFOIkQgzdydT9fOxPIaJi8OTZ6cf-_CQ

This project aims to develop an ML system that automatically processes meeting transcripts to:
1. Generate concise, coherent summaries of key discussion points
2. Extract specific action items with their assigned owners and deadlines
3. Categorize discussion segments by topic
"""

!pip install transformers datasets evaluate rouge_score spacy
!python -m spacy download en_core_web_sm
!pip install nltk
!python -m nltk.downloader punkt stopwords averaged_perceptron_tagger
!pip install sentence_transformers
!pip install wordcloud

# Necessary Libraries
import pandas as pd
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import re
from wordcloud import WordCloud, STOPWORDS
import nltk
from nltk.tokenize import sent_tokenize
from google.colab import drive
import os
import zipfile
import requests
from tqdm.notebook import tqdm
import pickle
from collections import defaultdict
from collections import Counter
from datetime import datetime, timedelta
import textwrap

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/mlminiproj

"""## 1. Data Loading + Preprocessing
Load AMI; build meeting -> [[utterance dicts]] map)
"""

dataset_dir = '/content/drive/MyDrive/mlminiproj/data'
!mkdir -p {dataset_dir}

from datasets import load_dataset

ami_dataset = load_dataset("edinburghcstr/ami", "ihm", trust_remote_code=True)
print(f"Dataset loaded successfully!")

save_dir = '/content/drive/MyDrive/mlminiproj/processed_data'
os.makedirs(save_dir, exist_ok=True)

meetings_by_id_split = defaultdict(list)

# Process each split at once
for split in ami_dataset.keys():
    print(f"Processing {split} split...")

    # Get all meeting IDs in this split
    meeting_ids = set(ami_dataset[split]['meeting_id'])
    print(f"  Found {len(meeting_ids)} meetings in {split} split")

    # Process all data in this split at once
    for i, item in enumerate(tqdm(ami_dataset[split])):
        # Skip audio data to save space
        processed_item = {
            'speaker': item['speaker_id'],
            'text': item['text'],
            'begin_time': item['begin_time'],
            'end_time': item['end_time'],
            'microphone_id': item['microphone_id'],
            'meeting_id': item['meeting_id']
        }

        # Store by meeting ID and split
        meetings_by_id_split[(item['meeting_id'], split)].append(processed_item)

        # Show progress every 10000 items
        if i % 10000 == 0 and i > 0:
            print(f"  Processed {i} items...")

# Sort each meeting by begin_time
print("Sorting meetings by timestamp...")
for key in meetings_by_id_split:
    meetings_by_id_split[key].sort(key=lambda x: x['begin_time'])

# Save the processed data
print("Saving processed data...")
with open(f"{save_dir}/ami_processed_meetings_fast.pkl", 'wb') as f:
    pickle.dump(dict(meetings_by_id_split), f)

print(f"Processed data saved to {save_dir}/ami_processed_meetings_fast.pkl")

#verifying processed data
save_dir = '/content/drive/MyDrive/mlminiproj/processed_data'
with open(f"{save_dir}/ami_processed_meetings_fast.pkl", 'rb') as f:
    meetings_data = pickle.load(f)

print(f"Loaded {len(meetings_data)} meeting instances")
print(f"Example meeting IDs: {list(meetings_data.keys())[:5]}")

# Examine one meeting
sample_key = list(meetings_data.keys())[0]
sample_meeting = meetings_data[sample_key]
print(f"\nSample meeting {sample_key} has {len(sample_meeting)} segments")
print("\nFirst 5 segments:")
for segment in sample_meeting[:5]:
  print(f"[{segment['speaker']}]: {segment['text']}")

print("Dataset structure:", ami_dataset)
print("Keys in training data:", list(ami_dataset['train'][0].keys()))

#full example
print("\nExample entry from the dataset:")
example = ami_dataset['train'][0]
for key, value in example.items():
  if key != 'audio':
    print(f"{key}: {value}")
  else:
    print(f"{key}: [AUDIO DATA]")

#unique meetings
unique_meeting = set(ami_dataset['train']['meeting_id'])
print(f"\nNumber of unique meetings in training set: {len(unique_meeting)}")
print(f"Sample meeting IDs: {list(unique_meeting)[:5]}")

#all segments from one meeting
meeting_id = list(unique_meeting)[0]
meeting_segments = [item for item in ami_dataset['train'] if item['meeting_id'] == meeting_id]
print(f"\nNumber of segments in meeting {meeting_id}: {len(meeting_segments)}")
print("f\nNumber of segments in meeting {meeting_id}: {len(meeting_segments)}")

#segments from this meeting
print("\nSample segments from this meeting:")
for segment in meeting_segments[:3]:
      print(f"Time: {segment['begin_time']}-{segment['end_time']}, Text: {segment['text']}")

#text cleaning and preprocessing

# disfluency / filler set
FILLERS = {
    'um','uh','hmm','yeah','right','so','okay','okay?', 'you know',
    'like','i mean','er','ah','mm','ahh','mmm','huh' 'kay', 'uhh', 'umm', 'well'
}

# filler regex (word boundaries)
FILLER_RE = re.compile(r'\b(' + '|'.join(map(re.escape, FILLERS)) + r')\b', flags=re.IGNORECASE)

# annotation regex (e.g. [laughter], (cough), [inaudible])
ANNOT_RE = re.compile(r'\[.*?\]|\(.*?\)')

# Contraction map
CONTRACTIONS = {
    "i'm":"i am", "we're":"we are", "it's":"it is", "they're":"they are",
    "can't":"cannot", "don't":"do not", "won't":"will not", # …etc.
}
CONTRACTION_RE = re.compile(
    r'\b(' + '|'.join(map(re.escape, CONTRACTIONS.keys())) + r')\b',
    flags=re.IGNORECASE
)

def clean_utterance(text):
    # a) lowercase
    txt = text.lower()
    # b) strip annotations
    txt = ANNOT_RE.sub(' ', txt)
    # c) expand contractions
    def _expand(m): return CONTRACTIONS[m.group(0)]
    txt = CONTRACTION_RE.sub(_expand, txt)
    # d) remove filler words
    txt = FILLER_RE.sub(' ', txt)
    # e) remove filler punctuation (ellipses, long dashes)
    txt = re.sub(r'[\.\-–—]{2,}', ' ', txt)
    # f) remove any remaining non‑alphanumeric (keep .?! if you need them)
    txt = re.sub(r'[^a-z0-9\s\.\?\!]', ' ', txt)
    # g) collapse whitespace
    txt = re.sub(r'\s+', ' ', txt).strip()
    return txt

# 5) Apply once, up‑front, to all segments
for key, segments in meetings_data.items():
    for seg in segments:
        seg['text_clean'] = clean_utterance(seg['text'])

"""## 2. Topic Segmentation
We first chunk each meeting transcript into coherent topical units using a sliding‑window TF–IDF approach that detects significant drops in cosine similarity.
"""

def segment_topics_improved(meeting_segments,
                            window_size: int = 25,
                            threshold: float = 0.3,
                            min_segment_size: int = 15):
    """
    Slide a window over the cleaned utterances, compute TF‑IDF similarity
    between adjacent windows, and break topics where similarity drops
    significantly below the mean by `threshold * std_dev`. Enforce a
    minimum segment size.
    """
    # Extract cleaned texts
    texts = [seg['text_clean'] for seg in meeting_segments]
    if len(texts) < window_size*2 or len(texts) < min_segment_size:
        return [meeting_segments]

    # 1) Build TF‑IDF matrix
    vect      = TfidfVectorizer(stop_words='english', min_df=2)
    tfidf_mat = vect.fit_transform(texts).toarray()

    # 2) Compute similarities between adjacent windows
    sims = []
    for i in range(len(texts) - 2*window_size + 1):
        win1 = tfidf_mat[i : i+window_size].mean(axis=0).reshape(1, -1)
        win2 = tfidf_mat[i+window_size : i+2*window_size].mean(axis=0).reshape(1, -1)
        sims.append(cosine_similarity(win1, win2)[0,0])

    # 3) Identify drops > threshold*std
    mean_sim = np.mean(sims)
    std_sim  = np.std(sims)
    potential = [
        i + window_size
        for i, val in enumerate(sims)
        if val < (mean_sim - threshold*std_sim)
    ]

    # 4) Enforce minimum segment size
    boundaries, prev = [], 0
    for b in potential:
        if b - prev >= min_segment_size:
            boundaries.append(b)
            prev = b

    # 5) Split into topic segments
    topics, start = [], 0
    for b in boundaries:
        topics.append(meeting_segments[start:b])
        start = b
    # last segment
    topics.append(meeting_segments[start:])

    return topics

"""### 2.1 Segmentation & Sanity Check"""

sample_key     = ('EN2001a','train')
sample_meeting = meetings_data[sample_key]

print(f"Segmenting meeting {sample_key}…")

improved_topics = segment_topics_improved(sample_meeting)

# Print counts & first utterance of each topic
lengths = [len(t) for t in improved_topics]
print(f"\nDetected {len(improved_topics)} topics; lengths = {lengths}\n")
for i, seg in enumerate(improved_topics, start=1):
    first = seg[0]['text_clean'][:60].replace('\n',' ')
    print(f" Topic {i:2d}: {len(seg):3d} utts, starts with “{first}…”")

"""### 2.2 Quantitative Summary

"""

lengths = np.array([len(t) for t in improved_topics])
print(f"\nAverage topic length      = {lengths.mean():.1f} utts")
print(f"Standard deviation       = {lengths.std():.1f} utts")
print(f"Shortest / Longest topics = {lengths.min()} / {lengths.max()} utts")

"""### 2.3 Timeline Chart of Topic Segments"""

intervals = [
    (seg[0]['begin_time'], seg[-1]['end_time'] - seg[0]['begin_time'])
    for seg in improved_topics
]

fig, ax = plt.subplots(figsize=(10, 1.5))
ax.broken_barh(intervals, (0,1), facecolors=plt.cm.tab20.colors[:len(intervals)])
ax.set_yticks([]); ax.set_xlabel("Time (s)"); ax.set_title("Topic Segmentation Timeline")

for idx, (st, length) in enumerate(intervals, start=1):
    ax.text(st + length/2, 0.5, f"T{idx}", ha='center', va='center',
            fontsize=6, color='white')

plt.tight_layout()
plt.savefig("topic_timeline.png", dpi=300, bbox_inches='tight')
plt.show()

"""### 2.4 Coherence Heatmap per Topic (SBERT Embeddings)"""

# encode every utterance once
model = SentenceTransformer("all-MiniLM-L6-v2")
all_texts = [seg['text_clean'] for segs in improved_topics for seg in segs]
embs      = model.encode(all_texts, show_progress_bar=True)

# compute avg pairwise sim per topic
coherences = []
offset = 0
for segs in improved_topics:
    n = len(segs)
    e = embs[offset:offset+n]
    offset += n
    sims = cosine_similarity(e, e)
    tri  = sims[np.triu_indices(n, k=1)]
    coherences.append(tri.mean() if len(tri)>0 else 1.0)

# Plot
fig, ax = plt.subplots(figsize=(10, 1.5))
cax = ax.imshow([coherences], aspect='auto', cmap='YlGnBu', vmin=0, vmax=1)
fig.colorbar(cax, label='Avg Sentence‑Sim. (0–1)', orientation='vertical', pad=0.02)
ax.set_xticks(range(len(coherences)))
ax.set_xticklabels([f"T{i}" for i in range(1, len(coherences)+1)],
                   rotation=90, fontsize=6)
ax.set_yticks([]); ax.set_title("Topic Coherence Heatmap")
plt.tight_layout()
plt.savefig("topic_coherence.png", dpi=300, bbox_inches='tight')
plt.show()

"""### 2.5 Word-Clouds (TF-IDF + Extended Stopwords + Hedges)"""

# stopword sets
fillers = {
    'um','uh','hmm','yeah','right','so','okay','you know','like',
    'er','ah','mm','huh', 'nite', 'n ite'
}
hedges = {
    'maybe','pretty','sure','think','looks','looked','probably',
    'actually','basically','really','quite','sort','mean','gonna', 'already',
    'sort of','kind of','type of','thing','well'
}
extra = {
    'utterance','meeting','display','level','stuff','data','file','alert','segment',
    'position','integer','type', 'tie'
}
stopwords = STOPWORDS.union(fillers).union(hedges).union(extra)

# Cleaning function: lowercase, remove fillers/hedges, single chars, punctuation, collapse spaces
def clean_text(text):
    txt = text.lower()
    # remove filler & hedging words
    txt = re.sub(r'\b(' + '|'.join(map(re.escape, fillers | hedges)) + r')\b', ' ', txt)
    # drop single‑character tokens
    txt = re.sub(r'\b[a-z]\b', ' ', txt)
    # keep only alphanumeric and spaces
    txt = re.sub(r'[^a-z0-9\s]', ' ', txt)
    # collapse whitespace
    return re.sub(r'\s+', ' ', txt).strip()

# cleaned corpus (one document per topic)
corpus = []
for segs in improved_topics:
    raw = " ".join(seg['text_clean'] for seg in segs)
    corpus.append(clean_text(raw))

# Compute TF‑IDF weights using list(stopwords)
vectorizer = TfidfVectorizer(stop_words=list(stopwords), max_features=200)
X       = vectorizer.fit_transform(corpus)   # shape: (n_topics, n_terms)
terms   = vectorizer.get_feature_names_out()

# Prepare per‑topic frequency dicts
freqs_list = []
for row in X.toarray():
    freqs_list.append({terms[i]: row[i] for i in range(len(terms)) if row[i] > 0})

# 6) Plot TF‑IDF word‑clouds
fig, axes = plt.subplots(2, 3, figsize=(12, 6))
for i, ax in enumerate(axes.flatten(), start=1):
    if i > len(freqs_list):
        ax.axis('off')
        continue
    wc = WordCloud(
        width=300, height=200,
        background_color='white',
        stopwords=stopwords
    ).generate_from_frequencies(freqs_list[i-1])

    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    ax.set_title(f"T{i} (n={len(improved_topics[i-1])} utts)", fontsize=8)

plt.tight_layout()
plt.savefig("topic_wordclouds_final.png", dpi=300, bbox_inches='tight')
plt.show()

"""---
## 2.6 Summary of Topic Segmentation

- **Detected:** 19 topics in meeting EN2001a (train), averaging 88.3 utterances per topic  
- **Timeline:** Clear boundaries visualized over the full meeting duration  
- **Coherence:** Measured via average sentence‐embedding similarity (0–1 scale)  
- **Key Terms:** TF‑IDF word‑clouds highlight distinctive content words per topic

With our transcript now neatly partitioned into topical units, we can proceed to **Section 3: Extractive Summarization**, where we’ll pull out the most salient utterances from each segment.

---

## 3. Extractive Summarization Component**
Within each topic segment, we pick the top N most “important” utterances by combining:
- **Content relevance** (TF–IDF similarity to the whole segment)  
- **Position** (earlier utterances weighted higher)  
- **Length** (very short ones down‑weighted)  
- **Action cues** (contains task keywords)  
- **Speaker novelty** (first time speaker appears)

### 3.1 Feature Extraction & Ranking Functions
"""

action_keywords = ['will', 'need to', 'going to', 'must', 'should', 'action', 'task', 'due']

def extract_features(segments):
    """Compute features for each utterance in a topic segment."""
    texts = [seg['text_clean'] for seg in segments]
    # 1) TF–IDF vectors (dense)
    vect  = TfidfVectorizer(stop_words='english')
    tfidf = vect.fit_transform(texts).toarray()       # shape (N, V)
    # 2) Document‐level average vector
    doc_vec = tfidf.mean(axis=0).reshape(1, -1)       # shape (1, V)
    # 3) Cosine similarity of each utterance to doc_vec
    sims = cosine_similarity(tfidf, doc_vec).flatten()  # shape (N,)

    features = []
    seen     = set()
    for i, seg in enumerate(segments):
        position    = i / max(len(segments)-1, 1)       # normalized 0→1
        length      = len(seg['text_clean'].split())
        speaker     = seg['speaker']
        is_new_spkr = speaker not in seen
        seen.add(speaker)
        contains_a  = any(kw in seg['text_clean'] for kw in action_keywords)

        features.append({
            'idx':           i,
            'content_sim':   sims[i],
            'position':      position,
            'length':        length,
            'is_new_spkr':   is_new_spkr,
            'contains_a':    contains_a
        })
    return features

def rank_segments(features, top_n=5):
    """
    Rank by weighted sum:
      0.3*content_sim + 0.2*(1-position) + 0.2*min(length/20,1)
    + 0.2*contains_action + 0.1*is_new_spkr
    """
    scores = []
    for f in features:
        sc = (
            0.3 * f['content_sim'] +
            0.2 * (1 - f['position']) +
            0.2 * min(f['length']/20, 1.0) +
            0.2 * int(f['contains_a']) +
            0.1 * int(f['is_new_spkr'])
        )
        scores.append((f['idx'], sc))
    ranked = sorted(scores, key=lambda x: x[1], reverse=True)
    return [idx for idx,_ in ranked[:top_n]]

def generate_extractive_summary(segments, top_n=5):
    """Return the top_n utterances (in original order) for a topic segment."""
    feats   = extract_features(segments)
    top_idxs = rank_segments(feats, top_n)
    top_idxs.sort()
    return [segments[i] for i in top_idxs]

"""### 3.2 Summarization per Topic"""

rows = []
for t_idx, topic in enumerate(improved_topics, start=1):
    summary = generate_extractive_summary(topic, top_n=3)
    for seg in summary:
        rows.append({
            'Topic':       t_idx,
            'Speaker':     seg['speaker'],
            'Utterance':   seg['text_clean'][:80] + ('…' if len(seg['text_clean'])>80 else '')
        })

df = pd.DataFrame(rows)
df

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df)

all_pos = []
for segs in improved_topics:
    feats    = extract_features(segs)
    top_idxs = rank_segments(feats, top_n=3)
    all_pos.extend([feats[i]['position'] for i in top_idxs])

plt.hist(all_pos, bins=10, edgecolor='k')
plt.xlabel("Normalized Position in Topic")
plt.ylabel("Frequency of Selection")
plt.title("Where Summaries Come From in Each Segment")
plt.show()

"""## 4. Action Item Extraction
Using our enhanced rule‐based extractor, we pull actionable tasks out of each topic segment, along with:

- **Owner resolution:** pronouns → speaker or “Team”  
- **Task text cleaning:** strip “um/uh/bah,” trim to 3+ words  
- **Deadline detection:** “today,” “in 3 days,” “next week” → standardized days  
- **Priority scoring:** high/medium/low via keyword lookup  
- **Confidence:** 0–100 based on pattern type, deadline, priority & specificity  

We’ll then visualize per‐topic confidence, the priority distribution, and tabulate our top tasks.

### Helper Functions:
"""

# 1) Time terms for deadline extraction
time_terms = {
    'today': 0,
    'tomorrow': 1,
    'next week': 7,
    'next month': 30
}

# 2) Priority keywords
priority_terms = {
    'high':   ['urgent', 'asap', 'as soon as possible', 'important', 'priority'],
    'medium': ['soon', 'when you can', 'later'],
    'low':    ['optional', 'if you have time']
}

# 3) Filler words to strip from task descriptions
filler_words = [r'\buh\b', r'\bum\b', r'\ber\b', r'\bah\b']

# 4) Pronoun mapping for owner resolution
pronoun_mapping = {
    'we': 'Team',
    'us': 'Team',
    'me': 'self',
    'i': 'self'
}

# 5) Action‑item regex patterns
action_patterns = [
    (r"\b(?:we need to|we should|we have to)\s+([^.?!]+)",       "explicit_action"),
    (r"\b(?:can you|could you please|could you)\s+([^.?!]+)\?",   "request"),
    (r"\bplease\s+([^.?!]+)",                                    "request"),
    (r"\b(?:let's|let us)\s+([^.?!]+)",                           "group_task"),
    (r"\b(?:i will|I'll)\s+([^.?!]+)",                            "self_task"),
    (r"\b(?:you will|you'll)\s+([^.?!]+)",                        "task_assigned")
]

# 6) Extract a deadline term (e.g. "today", "in 3 days")
def extract_deadline(text):
    lower = text.lower()
    for term, days in time_terms.items():
        if term in lower:
            return {'term': term, 'days': days}
    m = re.search(r'in (\d+) days?', lower)
    if m:
        return {'term': m.group(0), 'days': int(m.group(1))}
    return None

# 7) Estimate priority level from keywords
def estimate_priority(text):
    lower = text.lower()
    for level, keywords in priority_terms.items():
        for kw in keywords:
            if kw in lower:
                return level
    return 'normal'

# 8) Clean up a raw task string (remove filler words, extra spaces)
def clean_task(task_str):
    s = task_str.strip()
    for pat in filler_words:
        s = re.sub(pat, '', s, flags=re.IGNORECASE)
    s = re.sub(r'\s+', ' ', s).strip()
    return s.capitalize()

# 9) Resolve an owner name, mapping pronouns to speaker or Team
def resolve_owner(who, speaker):
    key    = who.lower().strip()
    mapped = pronoun_mapping.get(key)
    if mapped == 'Team':
        return 'Team'
    elif mapped == 'self':
        return speaker
    # fallback: if "this"/"that"/short word, assign back to speaker
    if len(who) <= 3 or key in ['this','that','it']:
        return speaker
    return who.capitalize()

# verb check to filter only actionable tasks
def is_actionable(text):
    tokens = nltk.word_tokenize(text)
    tags   = nltk.pos_tag(tokens)
    return any(tag.startswith('VB') for _, tag in tags)

# extractor (returns a flat, sorted list per topic)
def extract_action_items_enhanced(meeting_segments):
    action_items = []
    for seg in meeting_segments:
        text    = seg['text']
        speaker = seg['speaker']

        for pattern, p_type in action_patterns:
            matches = re.findall(pattern, text, flags=re.IGNORECASE)
            if not matches:
                continue

            # extract who / what
            if isinstance(matches[0], tuple):
                who, what = matches[0][0], matches[0][1]
            else:
                who = 'Team' if p_type=='group_task' else speaker
                what = matches[0] if isinstance(matches[0], str) else matches[0][0]

            what = clean_task(what)
            if len(what.split()) < 3 or not is_actionable(what):
                continue

            # enrich with deadline, priority, confidence
            deadline = extract_deadline(text)
            priority = estimate_priority(text)
            conf     = 25
            conf += {
                'explicit_action':50,
                'task_assigned':40,
                'deadline':35,
                'high_priority':35,
                'need_statement':30
            }.get(p_type, 25)
            if deadline:             conf += 15
            if priority=='high':     conf += 15
            elif priority=='medium': conf += 10
            if len(what.split())>=8: conf += 10
            conf = min(100, conf)
            if conf < 30:
                continue

            action_items.append({
                'speaker':    speaker,
                'owner':      resolve_owner(who, speaker),
                'task':       what,
                'priority':   priority,
                'deadline':   deadline,
                'confidence': conf
            })
            break  # only one action per segment

    # return flat list sorted by confidence
    return sorted(action_items, key=lambda x: x['confidence'], reverse=True)

# 3) Visualization helpers
def plot_confidence_heatmap(topic_actions):
    vals = [np.mean([a['confidence'] for a in acts]) if acts else 0
            for acts in topic_actions]
    plt.figure(figsize=(12,2))
    plt.imshow([vals], aspect='auto', cmap='YlGnBu')
    plt.colorbar(label='Avg Confidence')
    plt.xticks(range(len(vals)), [f"T{i+1}" for i in range(len(vals))], rotation=90)
    plt.yticks([])
    plt.title("Average Action‑Item Confidence by Topic")
    plt.tight_layout()
    plt.show()

def plot_priority_distribution(topic_actions):
    all_prios = [a['priority'] for acts in topic_actions for a in acts]
    counts    = Counter(all_prios)
    plt.figure(figsize=(5,3))
    plt.bar(counts.keys(), counts.values())
    plt.xlabel("Priority")
    plt.ylabel("Count")
    plt.title("Priority Distribution of Extracted Actions")
    plt.tight_layout()
    plt.show()

# 4) Usage: extract per topic, print top‑3, then visualize
enhanced_by_topic = [extract_action_items_enhanced(topic) for topic in improved_topics]

for t_idx, actions in enumerate(enhanced_by_topic):
    print(f"\nTOPIC {t_idx+1} – {len(actions)} action(s)")
    for j, a in enumerate(actions[:3]):   # top 3
        line = f" {j+1}. [{a['confidence']}%] {a['owner']}: {a['task']} (prio={a['priority']})"
        if a.get('deadline'):
            line += f"  – deadline: {a['deadline']['term']}"
        print(line)

plot_confidence_heatmap(enhanced_by_topic)
plot_priority_distribution(enhanced_by_topic)

"""### 4.1 Sanity Check on Single Topic"""

sample_topic = improved_topics[0]
tasks = extract_action_items_enhanced(sample_topic)

print(f"Topic 1 — Found {len(tasks)} action(s)")
for t in tasks:
    line = f"- [{t['confidence']}%] {t['owner']}: {t['task']} (prio={t['priority']})"
    if t.get('deadline'):
        line += f", deadline={t['deadline']['term']}"
    print(line)

print("\n=== TOP‑3 PER TOPIC (wrapped tasks) ===")
for t_idx, actions in enumerate(enhanced_by_topic, start=1):
    print(f"\nTOPIC {t_idx} – {len(actions)} action(s)")
    for j, a in enumerate(actions[:3]):
        wrapped = textwrap.fill(a['task'], width=60, subsequent_indent='    ')
        print(f" {j+1}. [{a['confidence']}%] {a['owner']}:")
        print(f"    {wrapped}  (prio={a['priority']})", end='')
        if a.get('deadline'):
            print(f"  — deadline: {a['deadline']['term']}")
        else:
            print()

# heatmap
plot_confidence_heatmap(enhanced_by_topic)

# priority bar chart
plot_priority_distribution(enhanced_by_topic)

# top-5 table
flat = []
for t_idx, actions in enumerate(enhanced_by_topic, start=1):
    for a in actions:
        a['topic_idx'] = t_idx
        flat.append(a)

top5 = sorted(flat, key=lambda x: x['confidence'], reverse=True)[:5]
df_top5 = pd.DataFrame(top5)[['topic_idx','owner','task','priority','confidence']]

print("\n=== TOP 5 ACTIONS OVERALL ===")
display(df_top5)

"""### 4.2 Batch Extraction & Interactive Task Table"""

# Extract action items across all topics
enhanced_by_topic = [
    extract_action_items_enhanced(topic)
    for topic in improved_topics
]

# Console summary: top-3 per topic
for t_idx, actions in enumerate(enhanced_by_topic, start=1):
    print(f"\nTopic {t_idx} — {len(actions)} action(s)")
    for a in actions[:3]:
        line = f"- [{a['confidence']}%] {a['owner']}: {a['task']} (prio={a['priority']})"
        if a.get('deadline'):
            line += f", deadline={a['deadline']['term']}"
        print(line)

#  DataFrame of all extracted tasks


rows = []
for t_idx, actions in enumerate(enhanced_by_topic, start=1):
    for rank, act in enumerate(actions, start=1):

      deadline_info = act.get('deadline') or {}
      deadline_term = deadline_info.get('term', '')

      rows.append({
            'Topic':      t_idx,
            'Rank':       rank,
            'Owner':      act['owner'],
            'Task':       act['task'],
            'Priority':   act['priority'],
            'Deadline':   deadline_term,
            'Confidence': act['confidence']
        })

df_tasks = pd.DataFrame(rows)

#  Display interactively in Colab
from google.colab import data_table
data_table.DataTable(df_tasks, include_index=False, num_rows_per_page=10)

"""## 5. End-to-End Demo
Here we pick a fresh meeting, run our full pipeline (segmentation → summarization → action‐item extraction), and display both summaries and tasks interactively.
"""

# 5) End-to-End Pipeline Demo

# — 5.1) Pick a demo meeting —
demo_key     = ('EN2001a','train')             # or any other meeting ID
demo_meeting = meetings_data[demo_key]
print(f"▶ Running pipeline on meeting {demo_key!r}, {len(demo_meeting)} segments\n")

# — 5.2) Topic Segmentation —
demo_topics = segment_topics_improved(demo_meeting)
print(f"➤ Detected {len(demo_topics)} topic segments\n")

# — 5.3) Extractive Summarization per Topic —
import pandas as pd
from google.colab import data_table

summ_rows = []
print("=== Summaries by Topic ===")
for t_idx, topic in enumerate(demo_topics, start=1):
    summary = generate_extractive_summary(topic, top_n=3)
    print(f"\nTopic {t_idx} — {len(summary)} utterance(s):")
    for seg in summary:
        text = seg.get('text_clean', seg['text'])  # use cleaned if available
        print(f"  • [{seg['speaker']}]: {text}")
        summ_rows.append({
            'Topic':   t_idx,
            'Speaker': seg['speaker'],
            'Summary': text
        })

# show interactive summary table
df_summ = pd.DataFrame(summ_rows)
print("\nInteractive summary table:")
data_table.DataTable(df_summ, include_index=False, num_rows_per_page=8)


# — 5.4) Action-Item Extraction per Topic —
act_rows = []
print("\n=== Action Items by Topic ===")
demo_actions = []
for t_idx, topic in enumerate(demo_topics, start=1):
    actions = extract_action_items_enhanced(topic)
    demo_actions.append(actions)
    print(f"\nTopic {t_idx} — {len(actions)} action(s):")
    for act in actions[:3]:
        dl = act.get('deadline') or {}
        line = f"  • [{act['confidence']}%] {act['owner']}: {act['task']} (prio={act['priority']})"
        if dl.get('term'):
            line += f", deadline={dl['term']}"
        print(line)
    # collect
    for rank, act in enumerate(actions, start=1):
        dl = act.get('deadline') or {}
        act_rows.append({
            'Topic':      t_idx,
            'Rank':       rank,
            'Owner':      act['owner'],
            'Task':       act['task'],
            'Priority':   act['priority'],
            'Deadline':   dl.get('term',''),
            'Confidence': act['confidence']
        })

# interactive action‐item table
df_act = pd.DataFrame(act_rows)
print("\nInteractive action‐item table:")
data_table.DataTable(df_act, include_index=False, num_rows_per_page=8)


# — 5.5) Visualize Confidence & Priority Distributions —
print("\nPlotting confidence heatmap and priority distribution…")
plot_confidence_heatmap(demo_actions)
plot_priority_distribution(demo_actions)